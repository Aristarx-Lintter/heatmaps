project:
  name: "Heatmaps"
  experiment_name: "4-crosses-upd-transcribation"
  hf_checkpoint: "Archistrax/Qwen2-5-VL-4-full-heat-checkpoints"

clearml:
  project_name: ${project.name}
  task_name: ${project.experiment_name}
  output_uri: false
  reuse_last_task_id: true

dataset:
  name: "Archistrax/processed_transcribations"
  kwargs:
    transcribation_feature_name: "upd_transcribation"
    calibration_feature_name: "model_description"
    calib_prob: 0.3
  # name: "set_eye_dataset_off"
  # name: "dried_heatmaps"

model:
  name: "Qwen/Qwen2.5-VL-3B-Instruct"
  kwargs:
    torch_dtype: "bfloat16"
    device_map: "cuda"
    attn_implementation: "flash_attention_2"
    trust_remote_code: true

trainer:
  output_dir: "./4-full_heat_qwen2.5-checkpoints"
  num_train_epochs: 50
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 12
  gradient_accumulation_steps: 2
  learning_rate: 9e-6
  weight_decay: 0.1
  lr_scheduler_type: "linear"
  warmup_steps: 20
  logging_steps: 2
  save_steps: 50
  save_strategy: "steps"
  bf16: true
  fp16: false
  gradient_checkpointing: true
  optim: "adamw_8bit"
  push_to_hub: true
  hub_model_id: ${project.hf_checkpoint}
  eval_strategy: "steps"
  eval_steps: 50
  remove_unused_columns: false
  dataloader_num_workers: 12
  # max_grad_norm: 1.0

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  bias: "none"
  target_modules:
    - "visual.merger.mlp.0"
    - "visual.merger.mlp.2"
    - "model.q_proj"
    - "model.k_proj"
    - "model.v_proj"
    - "model.o_proj"
    - "model.gate_proj"
    - "model.up_proj"
    - "model.down_proj"
