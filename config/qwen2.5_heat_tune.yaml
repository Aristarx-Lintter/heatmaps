project:
  name: "Heatmaps"
  experiment_name: "WarpInjection-to-latent-upd-transcribation"
  hf_checkpoint: "Archistrax/Qwen2-5-VL-tuned-checkpoints"

clearml:
  project_name: ${project.name}
  task_name: ${project.experiment_name}
  output_uri: false
  reuse_last_task_id: true

dataset:
  name: "Archistrax/processed_transcribations"
  kwargs:
    transcribation_feature_name: "transcribation"
    calibration_feature_name: "model_description"
    calib_prob: 0.3
  # name: "set_eye_dataset_off"
  # name: "dried_heatmaps"

model:
  name: "baked_heat_qwen"
  kwargs:
    torch_dtype: "bfloat16"
    device_map: "cuda"
    attn_implementation: "flash_attention_2"
    trust_remote_code: true

trainer:
  output_dir: "./full_heat_qwen2.5-checkpoints"
  num_train_epochs: 50
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 12
  gradient_accumulation_steps: 2
  learning_rate: 4e-6
  weight_decay: 0.01
  lr_scheduler_type: "linear"
  warmup_steps: 20
  logging_steps: 2
  save_steps: 50
  save_strategy: "steps"
  bf16: true
  fp16: false
  gradient_checkpointing: true
  optim: "adamw_8bit"
  push_to_hub: true
  hub_model_id: ${project.hf_checkpoint}
  eval_strategy: "steps"
  eval_steps: 50
  remove_unused_columns: false
  dataloader_num_workers: 12
  # max_grad_norm: 1.0

lora:
  r: 128
  lora_alpha: 128
  lora_dropout: 0.1
  bias: "none"
  target_modules:
    - "heat_embedding.linear1"
    - "heat_embedding.linear2"
    - "visual.merger.mlp.0"
    - "visual.merger.mlp.2"
    - "visual.layers.32.qkv"
    - "visual.layers.32.proj"
    - "visual.layers.32.q"
    - "visual.layers.32.kv"
    - "visual.layers.32.gate_proj"
    - "visual.layers.32.up_proj"
    - "visual.layers.32.down_proj"
    - "model.q_proj"
    - "model.k_proj"
    - "model.v_proj"
    - "model.o_proj"
    - "model.gate_proj"
    - "model.up_proj"
    - "model.down_proj"