{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be2ec886-b52f-46a9-8ea3-f48c20f33971",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072f5997-8f0d-4f6e-992e-91eccc2c8f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e07f2708aad49d8875d901e08c3e293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoProcessor, AutoConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "from src.qwen2_5.model import Qwen2_5_VLForConditionalGenerationWithHeatmap\n",
    "\n",
    "\n",
    "path = \"qwen-checkpoints\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(path)\n",
    "# config = AutoConfig.from_pretrained(path, trust_remote_code=True)\n",
    "# config.vision_config.latent_dim = 256\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGenerationWithHeatmap.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # config=config,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eca8fe-282f-41bd-b249-afcec94beef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "set_dataset_off = Dataset.load_from_disk(\"dried_heatmaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9782b7-ca66-43d3-9298-ccf454d014bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.dataset import DataCollator\n",
    "\n",
    "\n",
    "data_collator = DataCollator(processor).data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe78291f-6f06-46f4-a51f-181e26da84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = data_collator(set_dataset_off.select(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d7f6653-6d8f-419f-a1e5-96190bc7e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35f9c09-c298-4786-81d5-52d6b7132e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in items:\n",
    "    items[key] = items[key].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ce58d1-3a33-4db5-89c7-768d53e83b06",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|><|im_start|>\\n<|im_start|>user<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>', '<|endoftext|><|im_start|>\\n<|im_start|>user<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>', '<|endoftext|><|im_start|><|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n', '<|endoftext|><|im_start|>\\n<|im_start|>user<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>', '<|endoftext|><|im_start|>\\n<|endoftext|><|im_start|>\\n<|im_start|>\\n<|endoftext|><|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>', '<|endoftext|><|im_start|>\\n<|endoftext|><|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n', '<|endoftext|><|im_start|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>', '<|endoftext|><|im_start|>\\n<|im_start|>\\n<|im_start|>user<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>', '<|endoftext|><|im_start|>\\n<|im_start|>user<|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|><|im_end|>', '<|endoftext|><|im_start|>\\n<|endoftext|><|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n<|im_start|>\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[151643, 151643, 151643,  ..., 151645, 151645, 151645],\n",
       "         [151643, 151643, 151643,  ..., 151645, 151645, 151645],\n",
       "         [151643, 151643, 151643,  ...,    198, 151644,    198],\n",
       "         ...,\n",
       "         [151643, 151643, 151643,  ..., 151645, 151645, 151645],\n",
       "         [151643, 151643, 151643,  ..., 151645, 151645, 151645],\n",
       "         [151643, 151643, 151643,  ...,    198, 151644,    198]],\n",
       "        device='cuda:0'),\n",
       " [tensor([151643, 151644,    198, 151644,    872, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645], device='cuda:0'),\n",
       "  tensor([151643, 151644,    198, 151644,    872, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645], device='cuda:0'),\n",
       "  tensor([151643, 151644, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198], device='cuda:0'),\n",
       "  tensor([151643, 151644,    198, 151644,    872, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645], device='cuda:0'),\n",
       "  tensor([151643, 151644,    198, 151643, 151644,    198, 151644,    198, 151643,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644], device='cuda:0'),\n",
       "  tensor([151643, 151644,    198, 151643, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198], device='cuda:0'),\n",
       "  tensor([151643, 151644, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645], device='cuda:0'),\n",
       "  tensor([151643, 151644,    198, 151644,    198, 151644,    872, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645], device='cuda:0'),\n",
       "  tensor([151643, 151644,    198, 151644,    872, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645,\n",
       "          151645, 151645], device='cuda:0'),\n",
       "  tensor([151643, 151644,    198, 151643, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198, 151644,    198, 151644,    198, 151644,    198, 151644,\n",
       "             198, 151644,    198, 151644,    198, 151644,    198, 151644,    198,\n",
       "          151644,    198], device='cuda:0')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_outputs(inputs):\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, \n",
    "        # skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    print(output_text)\n",
    "    return generated_ids, generated_ids_trimmed\n",
    "\n",
    "gen_outputs(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f276c74-1f83-4328-b136-f99935815748",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoProcessor, AutoConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "from src.qwen2_5.model import Qwen2_5_VLForConditionalGenerationWithHeatmap\n",
    "\n",
    "\n",
    "path = \"Archistrax/Qwen2_5_VL\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ec2480-4738-4fae-a89a-821fcfc3e4c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class ResizeTensor:\n",
    "    \"\"\"\n",
    "    Простой класс-трансформ, который повторяет поведение F.interpolate(..., align_corners=False)\n",
    "    для тензора формата (C, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, size=(14, 14), mode='bilinear', align_corners=False):\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        tensor = tensor.unsqueeze(0)  # -> (1, C, H, W)\n",
    "        tensor = F.interpolate(\n",
    "            tensor, size=self.size,\n",
    "            mode=self.mode,\n",
    "            align_corners=self.align_corners\n",
    "        )  # -> (1, C, newH, newW)\n",
    "        return tensor.squeeze(0)      # -> (C, newH, newW)\n",
    "\n",
    "def get_heatmap_transformation(h, w):\n",
    "    return transforms.Compose([\n",
    "        transforms.Lambda(lambda img: img.convert(\"L\")),\n",
    "        transforms.ToTensor(),  # теперь тензор формата (1, H, W)\n",
    "        transforms.Lambda(lambda t: (t - t.mean()) / (t.std() + 1e-8)),\n",
    "        ResizeTensor(size=(int(h), int(w)), mode='bilinear', align_corners=False),\n",
    "        transforms.Lambda(lambda t: t.flatten())\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8bf7a34-a05a-4f5d-a72c-55f5f3c9c1d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "image_folder = \"./images/\" \n",
    "messages_template = lambda image, transcription: [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_folder + image,\n",
    "                \"resized_height\": 14*40,\n",
    "                \"resized_width\": 14*80,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": transcription},\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dca5050-8466-419d-b3f8-2ae0ccd340c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def find_substring(input_ids:torch.Tensor, ref_ids: List[int]):\n",
    "    start_index = -1\n",
    "    for i in range(len(input_ids) - len(ref_ids) + 1):\n",
    "        if input_ids[i : i + len(ref_ids)].tolist() == ref_ids:\n",
    "            start_index = i\n",
    "            break\n",
    "    if start_index == -1:\n",
    "        raise ValueError(\"Target sequence not found.\")\n",
    "    end_index = start_index + len(ref_ids)\n",
    "    return start_index, end_index\n",
    "\n",
    "\n",
    "def create_labels(input_ids: torch.Tensor, answers: List[str]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create labels for SFT training. It masks all tokens after the start token with excluding_probability\n",
    "    and after end token for the rest.\n",
    "    Args:\n",
    "        input_ids: ids from tokenizer output\n",
    "        start_token: token that indicates start of selected sequence (for example, simple talk)\n",
    "        end_token: token that indicates end of selected sequence\n",
    "        excluding_probability: probability of excluding simple talk from attention\n",
    "\n",
    "    Returns: tensor with masks  for each input_ids\n",
    "    \"\"\"\n",
    "\n",
    "    labels = torch.full_like(input_ids, fill_value=-100)\n",
    "    \n",
    "    for i, row in enumerate(input_ids):\n",
    "        start_index, end_index = find_substring(row, processor.tokenizer(answers[i], add_special_tokens=False)[\"input_ids\"])\n",
    "        labels[i, start_index:end_index] = row[start_index:end_index]\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f32513-2a95-4bfe-b5e3-338cf473b367",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "answer_template = \"<|im_start|>assistant\\n{ans_text}<|im_end|>\\n\"\n",
    "\n",
    "def process_injection(image_grid_thw, features):\n",
    "    heatmap_flat = []\n",
    "    for thw, feature in zip(image_grid_thw, features):\n",
    "        _, h, w = thw\n",
    "        transformation = get_heatmap_transformation(h/2, w/2)\n",
    "        heatmap_flat.append(transformation(feature[\"heatmap\"]).unsqueeze(1))\n",
    "\n",
    "    return torch.stack(heatmap_flat)\n",
    "\n",
    "\n",
    "def data_collator(features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "    if not features:\n",
    "        return {}\n",
    "\n",
    "    messages = []\n",
    "    answers = []\n",
    "    for feature in features:\n",
    "        messages.append(messages_template(feature[\"image\"], feature[\"transcribation\"]))\n",
    "        answers.append(answer_template.format(ans_text=feature[\"transcribation\"]))\n",
    "\n",
    "    texts = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    image_inputs, _ = process_vision_info(messages)\n",
    "    batch = processor(text=texts, images=image_inputs, padding=True, return_tensors=\"pt\")  # ['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw']\n",
    "\n",
    "    batch[\"labels\"] = create_labels(batch[\"input_ids\"], answers)\n",
    "    batch[\"heatmap_flat\"] = process_injection(batch[\"image_grid_thw\"], features)\n",
    "\n",
    "    return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b8de29-330c-4620-b7e0-0a0fee55b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.visual.post_merger_injector.heatmap_proj.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "\n",
    "set_dataset_off = Dataset.load_from_disk(\"dried_heatmaps\")\n",
    "# train_data = set_dataset_off.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66757dbc-af60-4a8f-a324-f62ad0147b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9da7a7-b2a0-40d4-ae3e-5eae93dee0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstraction_4.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_dataset_off[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f30c99b-9a5f-4540-a46b-03513579c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный размер датасета: 16380\n",
      "Поиск уникальных комбинаций (human_id, image)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462fd59df8974d41adb3cf7ecc2abb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер отфильтрованного датасета: 833\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm # Для индикатора прогресса\n",
    "\n",
    "# Предположим, ваш датасет загружен в переменную `dataset`\n",
    "# dataset = Dataset(...)\n",
    "dataset = set_dataset_off\n",
    "\n",
    "print(f\"Исходный размер датасета: {len(dataset)}\")\n",
    "\n",
    "# --- Способ 1: Итерация с отслеживанием (Хорошо для умеренных размеров) ---\n",
    "\n",
    "seen_combinations = set()\n",
    "indices_to_keep = []\n",
    "\n",
    "# Получаем доступ к нужным колонкам (это эффективно в datasets)\n",
    "human_ids = dataset['human_id']\n",
    "# ПРЕДПОЛОЖЕНИЕ: Значения в колонке 'image' являются сравнимыми и хэшируемыми\n",
    "# (например, строки с путями к файлам или уникальные ID изображений).\n",
    "# Если там сами объекты изображений (PIL/numpy), этот способ не сработает напрямую.\n",
    "image_identifiers = dataset['image']\n",
    "\n",
    "print(\"Поиск уникальных комбинаций (human_id, image)...\")\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    # Создаем кортеж для ключа (кортежи хэшируемы)\n",
    "    combination = (human_ids[i], image_identifiers[i])\n",
    "\n",
    "    if combination not in seen_combinations:\n",
    "        seen_combinations.add(combination)\n",
    "        indices_to_keep.append(i) # Сохраняем индекс первого встреченного уникального\n",
    "\n",
    "# Создаем новый датасет, выбирая только нужные строки по индексам\n",
    "unique_dataset = dataset.select(indices_to_keep)\n",
    "\n",
    "print(f\"Размер отфильтрованного датасета: {len(unique_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3d07a68-fe35-4e05-b8ee-ec7c626f157e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad77cadd3b274f59a46c00244bf388d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_dataset.save_to_disk(\"dried_heatmaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8a70ed8-8856-4e78-b780-a29d13fea516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2076/1975460957.py:55: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from trl import SFTTrainer, Trainer\n",
    "from transformers import TrainingArguments, TrainerCallback, Trainer\n",
    "# from clearml import Task\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-checkpoints\",\n",
    "    num_train_epochs=40,\n",
    "    per_device_train_batch_size=2, #8?\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\", #linear?\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    push_to_hub=True,\n",
    "    report_to=[],\n",
    "    hub_model_id=\"Archistrax/Qwen2_5_VL-checkpoints\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "\n",
    "# class ClearMLCallback(TrainerCallback):\n",
    "#     def __init__(self, task):\n",
    "#         self.task = task\n",
    "#         self.logger = task.get_logger()\n",
    "\n",
    "#     def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "#         if logs:\n",
    "#             for key, value in logs.items():\n",
    "#                 self.logger.report_scalar(title=\"Training\", series=key, value=value, iteration=state.global_step)\n",
    "\n",
    "\n",
    "# experiment_name = \"qwen2.5-full\"\n",
    "# task = Task.init(\n",
    "#     project_name=\"qwen2.5\",\n",
    "#     task_name=experiment_name,\n",
    "#     output_uri=False\n",
    "# )\n",
    "\n",
    "\n",
    "tokenizer = processor.tokenizer\n",
    "# clearml_callback = ClearMLCallback(task)\n",
    "# logging.getLogger(\"clearml\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    # callbacks=[clearml_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a37f654-124b-447c-8bcc-135f3f21a841",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [41/80 00:25 < 00:25, 1.54 it/s, Epoch 13.40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.563000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.433500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2236\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 2236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2237\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2243\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2549\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2550\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2554\u001b[0m )\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2556\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2559\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2561\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2562\u001b[0m ):\n\u001b[1;32m   2563\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3718\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3717\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3718\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3724\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3783\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3781\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3782\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3783\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3785\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/src/qwen2_5/model.py:161\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGenerationWithHeatmap.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, heatmap_flat)\u001b[0m\n\u001b[1;32m    158\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39madd(delta)\n\u001b[1;32m    159\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    175\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1195\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m-> 1195\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1207\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1208\u001b[0m         hidden_states,\n\u001b[1;32m   1209\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1215\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m   1216\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:489\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m         )\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    492\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    493\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:264\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    261\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 264\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1066\u001b[0m, in \u001b[0;36mQwen2_5_VLDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m        into the model\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m-> 1066\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m   1070\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m   1071\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m   1078\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:138\u001b[0m, in \u001b[0;36mQwen2RMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    136\u001b[0m input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    137\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 138\u001b[0m variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f540a74-85b4-4ea8-96fb-269127b2f79c",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d2b150-9d82-47bc-b79a-e866aebfa95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1f1d8180614cd09ad25d4a0e6bdae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2_5_VLForConditionalGenerationWithHeatmap were not initialized from the model checkpoint at ./Qwen2.5-VL-3B-Instruct and are newly initialized: ['visual.post_merger_injector.heatmap_proj.0.bias', 'visual.post_merger_injector.heatmap_proj.0.weight', 'visual.post_merger_injector.heatmap_proj.2.bias', 'visual.post_merger_injector.heatmap_proj.2.weight', 'visual.post_merger_injector.norm.bias', 'visual.post_merger_injector.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda\n",
      "Размер батча: 2\n",
      "Батч данных успешно получен.\n",
      "Батч перемещен на cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from vlm_injector_train import HeatmapInjectionExperiment\n",
    "from transformers import AutoProcessor, AutoConfig, PreTrainedModel, ProcessorMixin, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "CONFIG_PATH = 'config/qwen2.5_heat.yaml'\n",
    "\n",
    "if not os.path.exists(CONFIG_PATH):\n",
    "    print(f\"ОШИБКА: Конфигурационный файл не найден: {CONFIG_PATH}\")\n",
    "else:\n",
    "    experiment = HeatmapInjectionExperiment(CONFIG_PATH)\n",
    "    experiment.prepare_for_training()\n",
    "\n",
    "    # Переменные для удобства\n",
    "    model = experiment.model\n",
    "    processor = experiment.processor\n",
    "    tokenizer = processor.tokenizer\n",
    "    train_dataset = experiment.train_dataset\n",
    "    data_collator = experiment.data_collator\n",
    "    cfg = experiment.cfg # Доступ к параметрам конфига\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=experiment.model,\n",
    "        processing_class=experiment.processor,\n",
    "        train_dataset=experiment.train_dataset,\n",
    "        eval_dataset=experiment.eval_dataset,\n",
    "        data_collator=experiment.data_collator,\n",
    "        args=experiment.train_args,\n",
    "        # callbacks=[ClearMLCallback(experiment.task)]\n",
    "    )\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Настройка Устройства и DataLoader\n",
    "\n",
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")\n",
    "\n",
    "# Перемещаем модель на устройство (если не используется device_map)\n",
    "if not cfg.model.kwargs.get('device_map'):\n",
    "     model.to(device)\n",
    "     print(f\"Модель перемещена на {device}\")\n",
    "\n",
    "# Создаем DataLoader для получения батча\n",
    "# --- ВАЖНО: Установите batch_size как в вашем конфиге trainer! ---\n",
    "batch_size = cfg.trainer.get('per_device_train_batch_size', 1) # Укажите ваш batch_size\n",
    "print(f\"Размер батча: {batch_size}\")\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "\n",
    "try:\n",
    "    batch = next(iter(train_dataloader))\n",
    "    print(\"Батч данных успешно получен.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при получении батча: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "    print(f\"Батч перемещен на {device}\")\n",
    "except Exception as e:\n",
    "     print(f\"Ошибка при перемещении батча на {device}: {e}\")\n",
    "     raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd8779b9-5166-48a6-bdb1-1c79f2ca8659",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2_5_VisionTransformerPretrainedModel\n",
    "\n",
    "\n",
    "class PostMergerFiLMInjector(nn.Module):\n",
    "    def __init__(self, hidden_dim, intermediate_dim=None):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.norm = nn.RMSNorm(hidden_dim, eps=1e-4)\n",
    "\n",
    "        intermediate_dim = hidden_dim // 4 if intermediate_dim is None else intermediate_dim\n",
    "        self.heatmap_proj = nn.Sequential(\n",
    "            nn.Linear(1, intermediate_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(intermediate_dim, 2 * hidden_dim)\n",
    "        )\n",
    "\n",
    "        nn.init.zeros_(self.heatmap_proj[-1].bias)\n",
    "        nn.init.zeros_(self.heatmap_proj[-1].weight)\n",
    "\n",
    "    def forward(self, hidden_states, heatmap_values):\n",
    "        \"\"\"\n",
    "        hidden_states: (N, D)\n",
    "        heatmap_values: (N, 1)\n",
    "        \"\"\"\n",
    "        with torch.autocast(device_type=hidden_states.device.type, dtype=torch.float32):\n",
    "            normed_hidden_states = self.norm(hidden_states.float())\n",
    "        # normed_hidden_states = self.norm(hidden_states)  # (N, D)\n",
    "        gamma_beta = self.heatmap_proj(heatmap_values)  # (N, 2*D)\n",
    "        gamma, beta = torch.chunk(gamma_beta, 2, dim=-1)\n",
    "\n",
    "        modulated_states = (1 + gamma) * normed_hidden_states + beta\n",
    "        output_states = hidden_states + modulated_states\n",
    "\n",
    "        return output_states\n",
    "\n",
    "\n",
    "class PostMergerHeatmapInjector(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim=None):\n",
    "        super().__init__()\n",
    "        latent_dim = latent_dim or hidden_dim // 4\n",
    "        self.to_latent = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "        self.from_latent = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.heatmap_proj = nn.Sequential(\n",
    "            nn.Linear(1, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, heatmap_values):\n",
    "        \"\"\"\n",
    "        hidden_states: (N, D)\n",
    "        heatmap_values: (N, 1)\n",
    "        \"\"\"\n",
    "        z = self.to_latent(hidden_states)  # (N, latent)\n",
    "        h = self.heatmap_proj(heatmap_values)  # (N, latent)\n",
    "        z_combined = z + h  # (N, latent)\n",
    "        return self.from_latent(z_combined)  # (N, D)\n",
    "\n",
    "\n",
    "class Qwen2_5_VisionTransformerWithHeatmap(Qwen2_5_VisionTransformerPretrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # self.post_merger_injector = PostMergerHeatmapInjector(config.out_hidden_size, config.latent_dim)\n",
    "        self.post_merger_injector = PostMergerFiLMInjector(config.out_hidden_size, config.latent_dim)\n",
    "\n",
    "    def forward(self, hidden_states: Tensor, grid_thw: Tensor, heatmap_flat: Tensor = None) -> Tensor:\n",
    "        hidden_states = super().forward(hidden_states, grid_thw)  # (N, D)\n",
    "        if heatmap_flat is not None:\n",
    "            print(\"--- Проверка hidden_states ПЕРЕД инжектором ---\")\n",
    "            is_bad_before = torch.isnan(hidden_states) | torch.isinf(hidden_states)\n",
    "            if is_bad_before.any():\n",
    "                print(\"!!! ОБНАРУЖЕНЫ NaN/Inf В hidden_states ПЕРЕД ИНЖЕКТОРОМ !!!\")\n",
    "            else:\n",
    "                print(f\"Статистика hidden_states перед инжектором: \"\n",
    "                      f\"min={hidden_states.min().item():.4f}, \"\n",
    "                      f\"max={hidden_states.max().item():.4f}, \"\n",
    "                      f\"mean={hidden_states.mean().item():.4f}\")\n",
    "            # ---\n",
    "            hidden_states = self.post_merger_injector(hidden_states, heatmap_flat)  # (N, D)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5df20d-4b66-47b2-80a1-971c79719288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import Qwen2_5_VLConfig\n",
    "from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import (\n",
    "    Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLCausalLMOutputWithPast\n",
    ")\n",
    "from transformers.utils import logging\n",
    "\n",
    "# from src.qwen2_5.vae.heatmap import Qwen2_5_VisionTransformerWithHeatmap\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class Qwen2_5_VLForConditionalGenerationWithHeatmap(Qwen2_5_VLForConditionalGeneration):\n",
    "    def __init__(self, config: Qwen2_5_VLConfig):\n",
    "        super().__init__(config)\n",
    "        self.visual = Qwen2_5_VisionTransformerWithHeatmap._from_config(config.vision_config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: torch.LongTensor = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.LongTensor] = None,\n",
    "            past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "            inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "            labels: Optional[torch.LongTensor] = None,\n",
    "            use_cache: Optional[bool] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "            pixel_values: Optional[torch.Tensor] = None,\n",
    "            pixel_values_videos: Optional[torch.FloatTensor] = None,\n",
    "            image_grid_thw: Optional[torch.LongTensor] = None,\n",
    "            video_grid_thw: Optional[torch.LongTensor] = None,\n",
    "            rope_deltas: Optional[torch.LongTensor] = None,\n",
    "            cache_position: Optional[torch.LongTensor] = None,\n",
    "            second_per_grid_ts: Optional[torch.Tensor] = None,\n",
    "            heatmap_flat=None,\n",
    "    ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n",
    "        r\"\"\"\n",
    "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> import requests\n",
    "        >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "        >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "        >>> messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.embed_tokens(input_ids)\n",
    "            if pixel_values is not None:\n",
    "                pixel_values = pixel_values.type(self.visual.dtype)\n",
    "                heatmap_flat = heatmap_flat.reshape(-1, 1).type(self.visual.dtype)  # (b*h*w, 1) Apply flattening for native injection as human attention\n",
    "                image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw, heatmap_flat=heatmap_flat)\n",
    "                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n",
    "                n_image_features = image_embeds.shape[0]\n",
    "                if n_image_tokens != n_image_features:\n",
    "                    raise ValueError(\n",
    "                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n",
    "                    )\n",
    "\n",
    "                mask = input_ids == self.config.image_token_id\n",
    "                mask_unsqueezed = mask.unsqueeze(-1)\n",
    "                mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)\n",
    "                image_mask = mask_expanded.to(inputs_embeds.device)\n",
    "\n",
    "                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n",
    "\n",
    "            if pixel_values_videos is not None:\n",
    "                pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n",
    "                video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n",
    "                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n",
    "                n_video_features = video_embeds.shape[0]\n",
    "                if n_video_tokens != n_video_features:\n",
    "                    raise ValueError(\n",
    "                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n",
    "                    )\n",
    "\n",
    "                mask = input_ids == self.config.video_token_id\n",
    "                mask_unsqueezed = mask.unsqueeze(-1)\n",
    "                mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)\n",
    "                video_mask = mask_expanded.to(inputs_embeds.device)\n",
    "\n",
    "                video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
    "                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(inputs_embeds.device)\n",
    "\n",
    "        if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n",
    "            # calculate RoPE index once per generation in the pre-fill stage only\n",
    "            if (\n",
    "                    (cache_position is not None and cache_position[0] == 0)\n",
    "                    or self.rope_deltas is None\n",
    "                    or (past_key_values is None or past_key_values.get_seq_length() == 0)\n",
    "            ):\n",
    "                position_ids, rope_deltas = self.get_rope_index(\n",
    "                    input_ids,\n",
    "                    image_grid_thw,\n",
    "                    video_grid_thw,\n",
    "                    second_per_grid_ts,\n",
    "                    attention_mask,\n",
    "                )\n",
    "                self.rope_deltas = rope_deltas\n",
    "            # then use the prev pre-calculated rope-deltas to get the correct position ids\n",
    "            else:\n",
    "                batch_size, seq_length, _ = inputs_embeds.shape\n",
    "                delta = (\n",
    "                    (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n",
    "                    if cache_position is not None\n",
    "                    else 0\n",
    "                )\n",
    "                position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n",
    "                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n",
    "                if cache_position is not None:  # otherwise `deltas` is an int `0`\n",
    "                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n",
    "                position_ids = position_ids.add(delta)\n",
    "                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=None,\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Upcast to float if we need to compute the loss to avoid potential precision issues\n",
    "            logits = logits.float()\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            # logits_to_check = shift_logits  # Тензор перед подачей в лосс\n",
    "            #\n",
    "            # if torch.isnan(logits_to_check).any():\n",
    "            #     print(\"!!! ОБНАРУЖЕНЫ NaN В shift_logits ПЕРЕД ФУНКЦИЕЙ ПОТЕРЬ !!!\")\n",
    "            # elif torch.isinf(logits_to_check).any():\n",
    "            #     print(\"!!! ОБНАРУЖЕНЫ Inf В shift_logits ПЕРЕД ФУНКЦИЕЙ ПОТЕРЬ !!!\")\n",
    "            # else:\n",
    "            #     # Если нет NaN/Inf, проверим масштаб\n",
    "            #     print(f\"Статистика shift_logits перед лоссом: \"\n",
    "            #           f\"min={logits_to_check.min().item():.4f}, \"\n",
    "            #           f\"max={logits_to_check.max().item():.4f}, \"\n",
    "            #           f\"mean={logits_to_check.mean().item():.4f}\")\n",
    "            #\n",
    "            # # Также проверьте метки на всякий случай\n",
    "            # if torch.any((shift_labels != -100) & ((shift_labels < 0) | (shift_labels >= self.config.vocab_size))):\n",
    "            #     print(\"!!! ОБНАРУЖЕНЫ НЕКОРРЕКТНЫЕ ID В shift_labels (кроме -100) !!!\")\n",
    "\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return Qwen2_5_VLCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            rope_deltas=self.rope_deltas,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "975387b0-083f-4771-9a34-ef8e71ca1e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d92d53c4724c15a3612b539bf925a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2_5_VLForConditionalGenerationWithHeatmap were not initialized from the model checkpoint at Qwen/Qwen2.5-VL-3B-Instruct and are newly initialized: ['visual.post_merger_injector.heatmap_proj.0.bias', 'visual.post_merger_injector.heatmap_proj.0.weight', 'visual.post_merger_injector.heatmap_proj.2.bias', 'visual.post_merger_injector.heatmap_proj.2.weight', 'visual.post_merger_injector.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from src.qwen2_5.model import Qwen2_5_VLForConditionalGenerationWithHeatmap\n",
    "\n",
    "name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "hf_config = AutoConfig.from_pretrained(name, trust_remote_code=True)\n",
    "hf_config.vision_config.latent_dim = 512\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGenerationWithHeatmap.from_pretrained(\n",
    "    name,\n",
    "    config=hf_config,\n",
    "    # ignore_mismatched_sizes=True,\n",
    "    **cfg.model.kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13082858-f7c1-4e69-9ae0-fc6a9d36264d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Установлено 8 хуков.\n"
     ]
    }
   ],
   "source": [
    "forward_hook_outputs = {}\n",
    "backward_hook_grads = {}\n",
    "\n",
    "# --- Функция для forward hook ---\n",
    "def get_forward_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        # Сохраняем выход (можно сохранять и input)\n",
    "        # Клонируем и отсоединяем, чтобы не хранить весь граф вычислений\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            data_to_store = output.detach().clone()\n",
    "        elif isinstance(output, tuple): # Некоторые слои возвращают кортежи\n",
    "            data_to_store = tuple(o.detach().clone() if isinstance(o, torch.Tensor) else o for o in output)\n",
    "        else:\n",
    "            data_to_store = output # Если не тензор, сохраняем как есть\n",
    "\n",
    "        forward_hook_outputs[name] = data_to_store\n",
    "        # Проверка на NaN/Inf на выходе\n",
    "        if isinstance(data_to_store, torch.Tensor):\n",
    "             if torch.isnan(data_to_store).any():\n",
    "                 print(f\"!!! ПОЙМАН NaN В FORWARD HOOK '{name}' (ВЫХОД) !!!\")\n",
    "             elif torch.isinf(data_to_store).any():\n",
    "                 print(f\"!!! ПОЙМАН Inf В FORWARD HOOK '{name}' (ВЫХОД) !!!\")\n",
    "        elif isinstance(data_to_store, tuple):\n",
    "             for i, o in enumerate(data_to_store):\n",
    "                 if isinstance(o, torch.Tensor):\n",
    "                     if torch.isnan(o).any():\n",
    "                         print(f\"!!! ПОЙМАН NaN В FORWARD HOOK '{name}' (ВЫХОД, элемент {i}) !!!\")\n",
    "                     elif torch.isinf(o).any():\n",
    "                         print(f\"!!! ПОЙМАН Inf В FORWARD HOOK '{name}' (ВЫХОД, элемент {i}) !!!\")\n",
    "\n",
    "    return hook\n",
    "\n",
    "# --- Функция для backward hook (регистрируем на ВЫХОД) ---\n",
    "# register_full_backward_hook ловит grad_input и grad_output модуля\n",
    "def get_backward_hook(name):\n",
    "    def hook(module, grad_input, grad_output):\n",
    "        # grad_output - градиент, пришедший К ЭТОМУ слою СВЕРХУ (ближе к лоссу)\n",
    "        # grad_input - градиент, который уйдет ОТ ЭТОГО слоя ВНИЗ (к входам)\n",
    "        data_to_store_in = tuple(g.detach().clone() if isinstance(g, torch.Tensor) else g for g in grad_input)\n",
    "        data_to_store_out = tuple(g.detach().clone() if isinstance(g, torch.Tensor) else g for g in grad_output)\n",
    "        backward_hook_grads[name] = {'grad_input': data_to_store_in, 'grad_output': data_to_store_out}\n",
    "\n",
    "        # Проверяем grad_output (градиенты, пришедшие к слою)\n",
    "        for i, g_out in enumerate(data_to_store_out):\n",
    "             if isinstance(g_out, torch.Tensor):\n",
    "                 if torch.isnan(g_out).any():\n",
    "                     print(f\"!!! ПОЙМАН NaN В BACKWARD HOOK '{name}' (GRAD_OUTPUT {i}) !!!\")\n",
    "                 elif torch.isinf(g_out).any():\n",
    "                     print(f\"!!! ПОЙМАН Inf В BACKWARD HOOK '{name}' (GRAD_OUTPUT {i}) !!!\")\n",
    "\n",
    "        # Проверяем grad_input (градиенты, уходящие от слоя)\n",
    "        for i, g_in in enumerate(data_to_store_in):\n",
    "             if isinstance(g_in, torch.Tensor):\n",
    "                 if torch.isnan(g_in).any():\n",
    "                     print(f\"!!! ПОЙМАН NaN В BACKWARD HOOK '{name}' (GRAD_INPUT {i}) !!!\")\n",
    "                 elif torch.isinf(g_in).any():\n",
    "                     print(f\"!!! ПОЙМАН Inf В BACKWARD HOOK '{name}' (GRAD_INPUT {i}) !!!\")\n",
    "\n",
    "    return hook\n",
    "\n",
    "# --- Регистрируем хуки на ключевые модули ---\n",
    "hooks = []\n",
    "target_modules = {\n",
    "    \"injector\": model.visual.post_merger_injector,\n",
    "    # Добавьте слои ВНУТРИ инжектора, если нужно детальнее\n",
    "    \"injector_norm\": model.visual.post_merger_injector.norm,\n",
    "    \"injector_heatmap_proj\": model.visual.post_merger_injector.heatmap_proj,\n",
    "    # \"injector_final_linear\": model.visual.post_merger_injector.from_latent[-1], # Пример\n",
    "    \"final_logits_layer\": model.lm_head # Слой, производящий финальные логиты\n",
    "}\n",
    "\n",
    "for name, module in target_modules.items():\n",
    "    if module is not None:\n",
    "        hooks.append(module.register_forward_hook(get_forward_hook(name)))\n",
    "        hooks.append(module.register_full_backward_hook(get_backward_hook(name))) # Используем register_full_backward_hook\n",
    "    else:\n",
    "        print(f\"Предупреждение: Модуль '{name}' не найден для установки хуков.\")\n",
    "\n",
    "print(f\"Установлено {len(hooks)} хуков.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "563e02f7-fd87-4551-9382-2d4284ddf58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.visual.post_merger_injector.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eece08b1-f0ee-4713-a316-cfffd048ddf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется torch.optim.AdamW с lr=1e-05\n",
      "Используется Autocast с dtype=torch.bfloat16?: True\n"
     ]
    }
   ],
   "source": [
    "learning_rate = cfg.trainer.get('learning_rate', 1e-6)\n",
    "optim_name = cfg.trainer.get('optim', 'adamw_8bit') # Используем adamw_torch для простоты, если 8bit вызывает проблемы\n",
    "\n",
    "if optim_name == 'adamw_8bit' and 'bitsandbytes' in globals():\n",
    "     try:\n",
    "         import bitsandbytes.optim as bnb_optim\n",
    "         optimizer = bnb_optim.AdamW8bit(model.parameters(), lr=learning_rate)\n",
    "         print(f\"Используется AdamW8bit с lr={learning_rate}\")\n",
    "     except ImportError:\n",
    "         print(\"bitsandbytes не найден, использую torch.optim.AdamW\")\n",
    "         optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "     print(f\"Используется torch.optim.AdamW с lr={learning_rate}\")\n",
    "\n",
    "\n",
    "# Включаем детектор аномалий (замедляет, но дает лучший трейсбек для NaN в backward)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Определяем контекст autocast в зависимости от конфига\n",
    "dtype_context = torch.bfloat16 if cfg.trainer.get('bf16', False) else torch.float32\n",
    "use_amp = (dtype_context != torch.float32)\n",
    "print(f\"Используется Autocast с dtype={dtype_context}?: {use_amp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2dd6004-4b22-45e3-81d8-3c621a000f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Выполнение Forward Pass ---\n",
      "--- Проверка hidden_states ПЕРЕД инжектором ---\n",
      "Статистика hidden_states перед инжектором: min=-69.0000, max=32.5000, mean=0.0104\n",
      "!!! ПОЙМАН NaN В FORWARD HOOK 'injector_norm' (ВЫХОД) !!!\n",
      "!!! ПОЙМАН NaN В FORWARD HOOK 'injector' (ВЫХОД) !!!\n",
      "!!! ПОЙМАН NaN В FORWARD HOOK 'final_logits_layer' (ВЫХОД) !!!\n",
      "Forward pass выполнен. Loss: nan\n",
      "!!! ПОЙМАН NaN В ИТОГОВОМ ЛОССЕ !!!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Выполнение Forward Pass ---\")\n",
    "model.train() # Переводим модель в режим обучения\n",
    "optimizer.zero_grad() # Обнуляем градиенты перед forward/backward\n",
    "\n",
    "try:\n",
    "    with torch.autocast(device_type=device.type, dtype=dtype_context, enabled=use_amp):\n",
    "        # Передаем батч в модель. Модель должна вернуть словарь, включающий 'loss'\n",
    "        outputs = model(**batch)\n",
    "        # Извлекаем лосс\n",
    "        if isinstance(outputs, dict) and 'loss' in outputs:\n",
    "             loss = outputs['loss']\n",
    "             print(f\"Forward pass выполнен. Loss: {loss.item():.4f}\")\n",
    "        else:\n",
    "             # Если модель не возвращает лосс, его нужно вычислить вручную\n",
    "             # logits = outputs.logits # Или как они у вас называются\n",
    "             # shift_logits = logits[..., :-1, :].contiguous()\n",
    "             # shift_labels = batch['labels'][..., 1:].contiguous()\n",
    "             # loss_fct = nn.CrossEntropyLoss()\n",
    "             # loss = loss_fct(shift_logits.view(-1, model.config.vocab_size), shift_labels.view(-1))\n",
    "             print(\"ПРЕДУПРЕЖДЕНИЕ: Модель не вернула 'loss'. Вычисление лосса не реализовано в этом скрипте отладки.\")\n",
    "             loss = None # Установим в None, чтобы пропустить backward\n",
    "\n",
    "    # Проверка лосса на NaN/Inf\n",
    "    if loss is not None:\n",
    "         if torch.isnan(loss):\n",
    "             print(\"!!! ПОЙМАН NaN В ИТОГОВОМ ЛОССЕ !!!\")\n",
    "         elif torch.isinf(loss):\n",
    "             print(\"!!! ПОЙМАН Inf В ИТОГОВОМ ЛОССЕ !!!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ОШИБКА во время Forward Pass: {e}\")\n",
    "    loss = None # Не можем делать backward\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bf2a50f-4bf3-4666-acbe-f50dbca5c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Выполнение Backward Pass ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Forward Pass ---\n",
    "\n",
    "\n",
    "# --- Backward Pass ---\n",
    "print(\"\\n--- Выполнение Backward Pass ---\")\n",
    "if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):\n",
    "    try:\n",
    "        # Без GradScaler для BF16 или FP32. Trainer использует accelerate.backward\n",
    "        # Для простоты отладки используем прямой вызов backward()\n",
    "        loss.backward()\n",
    "        print(\"Backward pass выполнен.\")\n",
    "\n",
    "        # --- Проверка Градиентов Параметров ---\n",
    "        print(\"\\n--- Проверка Градиентов Обучаемых Параметров ---\")\n",
    "        nan_found_in_grads = False\n",
    "        inf_found_in_grads = False\n",
    "        zero_grad_count = 0\n",
    "        non_zero_grad_count = 0\n",
    "\n",
    "        # Проверяем градиенты только для обучаемых параметров инжектора\n",
    "        injector_params = model.visual.post_merger_injector.parameters()\n",
    "        for name, param in model.visual.post_merger_injector.named_parameters():\n",
    "             if param.requires_grad:\n",
    "                 if param.grad is None:\n",
    "                     print(f\"  Градиент для '{name}' равен None!\")\n",
    "                     zero_grad_count += 1\n",
    "                 else:\n",
    "                     if torch.isnan(param.grad).any():\n",
    "                         print(f\"!!! ПОЙМАН NaN В ГРАДИЕНТЕ '{name}' !!!\")\n",
    "                         nan_found_in_grads = True\n",
    "                     elif torch.isinf(param.grad).any():\n",
    "                         print(f\"!!! ПОЙМАН Inf В ГРАДИЕНТЕ '{name}' !!!\")\n",
    "                         inf_found_in_grads = True\n",
    "                     elif torch.count_nonzero(param.grad) == 0:\n",
    "                          # print(f\"  Градиент для '{name}' полностью нулевой.\")\n",
    "                          zero_grad_count += 1\n",
    "                     else:\n",
    "                          # print(f\"  Градиент для '{name}' OK (ненулевой). Mean abs: {param.grad.abs().mean().item():.4g}\")\n",
    "                          non_zero_grad_count += 1\n",
    "             # else: # Можно раскомментировать для проверки замороженных\n",
    "             #     if param.grad is not None:\n",
    "             #         print(f\"ПРЕДУПРЕЖДЕНИЕ: Градиент НЕ None для замороженного параметра '{name}'\")\n",
    "        print(f\"\"\"Итог проверки градиентов: NaN={nan_found_in_grads}, Inf={inf_found_in_grads}, Нулевых/None={zero_grad_count}, Ненулевых={non_zero_grad_count}\"\"\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"ОШИБКА во время Backward Pass: {e}\")\n",
    "        print(\"Вывод данных из хуков может помочь найти причину.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Неожиданная ОШИБКА во время Backward Pass: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c215e0e2-dd11-4432-901f-7650a06dd37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Выходные данные Forward Хуков ---\n",
      "\n",
      "Модуль: injector_norm\n",
      "  Shape: torch.Size([1600, 2048]), Dtype: torch.float32, Device: cuda:0\n",
      "  !!! Содержит NaN или Inf !!!\n",
      "\n",
      "Модуль: injector_heatmap_proj\n",
      "  Shape: torch.Size([1600, 4096]), Dtype: torch.bfloat16, Device: cuda:0\n",
      "  Stats: min=-0.2188, max=0.2246, mean=0.0000\n",
      "\n",
      "Модуль: injector\n",
      "  Shape: torch.Size([1600, 2048]), Dtype: torch.float32, Device: cuda:0\n",
      "  !!! Содержит NaN или Inf !!!\n",
      "\n",
      "Модуль: final_logits_layer\n",
      "  Shape: torch.Size([2, 849, 151936]), Dtype: torch.bfloat16, Device: cuda:0\n",
      "  !!! Содержит NaN или Inf !!!\n",
      "\n",
      "--- Данные Backward Хуков ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Выходные данные Forward Хуков ---\")\n",
    "for name, output_data in forward_hook_outputs.items():\n",
    "    print(f\"\\nМодуль: {name}\")\n",
    "    if isinstance(output_data, torch.Tensor):\n",
    "        print(f\"  Shape: {output_data.shape}, Dtype: {output_data.dtype}, Device: {output_data.device}\")\n",
    "        if not torch.isnan(output_data).any() and not torch.isinf(output_data).any():\n",
    "             print(f\"  Stats: min={output_data.min().item():.4f}, max={output_data.max().item():.4f}, mean={output_data.mean().item():.4f}\")\n",
    "        else:\n",
    "             print(f\"  !!! Содержит NaN или Inf !!!\")\n",
    "    elif isinstance(output_data, tuple):\n",
    "         print(f\"  Кортеж с {len(output_data)} элементами:\")\n",
    "         for i, o in enumerate(output_data):\n",
    "              if isinstance(o, torch.Tensor):\n",
    "                  print(f\"    Элемент {i}: Shape={o.shape}, Dtype={o.dtype}\")\n",
    "                  if not torch.isnan(o).any() and not torch.isinf(o).any():\n",
    "                       print(f\"      Stats: min={o.min().item():.4f}, max={o.max().item():.4f}, mean={o.mean().item():.4f}\")\n",
    "                  else:\n",
    "                       print(f\"      !!! Содержит NaN или Inf !!!\")\n",
    "              else:\n",
    "                   print(f\"    Элемент {i}: Тип={type(o)}\")\n",
    "    else:\n",
    "        print(f\"  Тип: {type(output_data)}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Данные Backward Хуков ---\")\n",
    "for name, grad_data in backward_hook_grads.items():\n",
    "    print(f\"\\nМодуль: {name}\")\n",
    "    print(\"  Grad Output (Градиенты, пришедшие к слою):\")\n",
    "    for i, g_out in enumerate(grad_data['grad_output']):\n",
    "        if isinstance(g_out, torch.Tensor):\n",
    "            print(f\"    Элемент {i}: Shape={g_out.shape}, Dtype={g_out.dtype}\")\n",
    "            if not torch.isnan(g_out).any() and not torch.isinf(g_out).any():\n",
    "                 print(f\"      Stats: min={g_out.min().item():.4g}, max={g_out.max().item():.4g}, mean={g_out.mean().item():.4g}\")\n",
    "            else:\n",
    "                 print(f\"      !!! Содержит NaN или Inf !!!\")\n",
    "        else:\n",
    "             print(f\"    Элемент {i}: {type(g_out)}\") # Часто None для входов, не требующих градиент\n",
    "\n",
    "    print(\"  Grad Input (Градиенты, уходящие от слоя):\")\n",
    "    for i, g_in in enumerate(grad_data['grad_input']):\n",
    "         if isinstance(g_in, torch.Tensor):\n",
    "             print(f\"    Элемент {i}: Shape={g_in.shape}, Dtype={g_in.dtype}\")\n",
    "             if not torch.isnan(g_in).any() and not torch.isinf(g_in).any():\n",
    "                  print(f\"      Stats: min={g_in.min().item():.4g}, max={g_in.max().item():.4g}, mean={g_in.mean().item():.4g}\")\n",
    "             else:\n",
    "                  print(f\"      !!! Содержит NaN или Inf !!!\")\n",
    "         else:\n",
    "              print(f\"    Элемент {i}: {type(g_in)}\") # Может быть None для не-тензорных входов или входов, не требующих градиент\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763c9f3a-21bf-4e1a-adab-63b4fb3ba16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm Weights: nan\n"
     ]
    }
   ],
   "source": [
    "print(\"LayerNorm Weights:\", model.visual.post_merger_injector.norm.weight.abs().mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31ac1c0b-9a1b-453c-a7d0-3cdba4782d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSNorm((2048,), eps=0.0001, elementwise_affine=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual.post_merger_injector.norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18b6db-812b-464d-b92a-7764778fec2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
