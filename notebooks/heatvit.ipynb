{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm_injector_train import HeatmapInjectionExperiment\n",
    "\n",
    "config = \"/workspace/config/qwen2.5_heat.yaml\"\n",
    "experiment = HeatmapInjectionExperiment(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67889399ca34edea257fb400f04d62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2_5_VLForConditionalGenerationWithHeatmap were not initialized from the model checkpoint at ./Qwen2.5-VL-3B-Instruct and are newly initialized: ['heat_embedding.linear1.bias', 'heat_embedding.linear1.weight', 'heat_embedding.linear2.bias', 'heat_embedding.linear2.weight', 'visual.blocks.32.attn.kv_proj.bias', 'visual.blocks.32.attn.kv_proj.weight', 'visual.blocks.32.attn.proj.bias', 'visual.blocks.32.attn.proj.weight', 'visual.blocks.32.attn.q_proj.bias', 'visual.blocks.32.attn.q_proj.weight', 'visual.blocks.32.mlp.down_proj.bias', 'visual.blocks.32.mlp.down_proj.weight', 'visual.blocks.32.mlp.gate_proj.bias', 'visual.blocks.32.mlp.gate_proj.weight', 'visual.blocks.32.mlp.up_proj.bias', 'visual.blocks.32.mlp.up_proj.weight', 'visual.blocks.32.norm0.weight', 'visual.blocks.32.norm1.weight', 'visual.blocks.32.norm2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from src.qwen2_5.fa_model import Qwen2_5_VLForConditionalGenerationWithHeatmap\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoConfig, PreTrainedModel, ProcessorMixin, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import (\n",
    "    Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLCausalLMOutputWithPast\n",
    ")\n",
    "\n",
    "hf_config = AutoConfig.from_pretrained(experiment.cfg.model.name, trust_remote_code=True)\n",
    "hf_config.vision_config.latent_dim = 512\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGenerationWithHeatmap.from_pretrained(\n",
    "    experiment.cfg.model.name,\n",
    "    config=hf_config,\n",
    "    # ignore_mismatched_sizes=True,\n",
    "    **experiment.cfg.model.kwargs\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(experiment.cfg.model.name)\n",
    "processor.tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoProcessor, AutoConfig, PreTrainedModel, ProcessorMixin, TrainingArguments, Trainer\n",
    "# from datasets import Dataset\n",
    "# from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import (\n",
    "#     Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLCausalLMOutputWithPast\n",
    "# )\n",
    "\n",
    "# hf_config = AutoConfig.from_pretrained(experiment.cfg.model.name, trust_remote_code=True)\n",
    "# hf_config.vision_config.latent_dim = 512\n",
    "\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     experiment.cfg.model.name,\n",
    "#     config=hf_config,\n",
    "#     # ignore_mismatched_sizes=True,\n",
    "#     **experiment.cfg.model.kwargs\n",
    "# )\n",
    "# processor = AutoProcessor.from_pretrained(experiment.cfg.model.name)\n",
    "# processor.tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.dataset import DataCollator\n",
    "\n",
    "experiment.prepare_dataset()\n",
    "data_collator = DataCollator(processor).data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = experiment.train_dataset.select(range(5))\n",
    "# batch = data_collator(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from src.common.templates import messages_template, answer_template\n",
    "from src.common.transforms import get_heatmap_transformation\n",
    "\n",
    "\n",
    "def find_substring(input_ids: torch.Tensor, ref_ids: List[int]):\n",
    "    start_index = -1\n",
    "    for i in range(len(input_ids) - len(ref_ids) + 1):\n",
    "        if input_ids[i: i + len(ref_ids)].tolist() == ref_ids:\n",
    "            start_index = i\n",
    "            break\n",
    "    if start_index == -1:\n",
    "        raise ValueError(\"Target sequence not found.\")\n",
    "    end_index = start_index + len(ref_ids)\n",
    "    return start_index, end_index\n",
    "\n",
    "\n",
    "def create_labels(input_ids: torch.Tensor, answers: List[str], tokenizer: PreTrainedTokenizer) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create labels for SFT training. It masks all tokens after the start token with excluding_probability\n",
    "    and after end token for the rest.\n",
    "    Args:\n",
    "        input_ids: ids from tokenizer output\n",
    "        ....\n",
    "\n",
    "    Returns: tensor with masks  for each input_ids\n",
    "    \"\"\"\n",
    "\n",
    "    labels = torch.full_like(input_ids, fill_value=-100)\n",
    "\n",
    "    for i, row in enumerate(input_ids):\n",
    "        start_index, end_index = find_substring(\n",
    "            row, tokenizer(answers[i], add_special_tokens=False)[\"input_ids\"]\n",
    "        )\n",
    "        labels[i, start_index:end_index] = row[start_index:end_index]\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def process_injection(image_grid_thw, features):\n",
    "    heatmap_flat = []\n",
    "    for thw, feature in zip(image_grid_thw, features):\n",
    "        _, h, w = thw\n",
    "        transformation = get_heatmap_transformation(h, w)\n",
    "        heatmap_flat.append(transformation(feature[\"heatmap\"]).unsqueeze(1))\n",
    "\n",
    "    return torch.stack(heatmap_flat)\n",
    "\n",
    "\n",
    "class DataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def data_collator(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        if not features:\n",
    "            return {}\n",
    "\n",
    "        messages = []\n",
    "        answers = []\n",
    "        for feature in features:\n",
    "            messages.append(messages_template(feature[\"image\"], feature[\"transcribation\"]))\n",
    "            answers.append(answer_template.format(ans_text=feature[\"transcribation\"]))\n",
    "\n",
    "        texts = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        image_inputs, _ = process_vision_info(messages)\n",
    "        batch = self.processor(\n",
    "            text=texts, images=image_inputs, padding=True, return_tensors=\"pt\"\n",
    "        )  # ['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw']\n",
    "\n",
    "        batch[\"labels\"] = create_labels(batch[\"input_ids\"], answers, self.processor.tokenizer)\n",
    "        batch[\"heatmap_flat\"] = process_injection(batch[\"image_grid_thw\"], features)\n",
    "\n",
    "        return batch\n",
    "data_collator = DataCollator(processor).data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossAttn: attn_output shape after flash_attn: torch.Size([16000, 16, 80])\n",
      "CrossAttn: final attn_output shape after projection: torch.Size([16000, 1280])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLCausalLMOutputWithPast(loss=tensor(3.3421, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[11.0000, 12.1875, 10.8750,  ...,  1.4297,  1.4297,  1.4297],\n",
       "         [11.0000, 17.6250, 17.5000,  ...,  8.5625,  8.5625,  8.5625],\n",
       "         [10.3750, 15.3125,  9.5000,  ...,  6.9062,  6.9062,  6.9062],\n",
       "         ...,\n",
       "         [10.9375,  7.6250,  9.4375,  ...,  3.5000,  3.5000,  3.5000],\n",
       "         [10.4375,  3.6562,  3.3125,  ...,  1.0859,  1.0859,  1.0859],\n",
       "         [ 6.4688,  4.0312, -0.9492,  ..., -0.1016, -0.1016, -0.1016]],\n",
       "\n",
       "        [[11.0000, 17.6250, 17.5000,  ...,  8.5625,  8.5625,  8.5625],\n",
       "         [10.3750, 15.3125,  9.5000,  ...,  6.9062,  6.9062,  6.9062],\n",
       "         [15.5000, 18.3750, 20.8750,  ...,  7.0000,  7.0000,  7.0000],\n",
       "         ...,\n",
       "         [10.5000,  7.1875,  8.3750,  ...,  3.2656,  3.2656,  3.2656],\n",
       "         [10.2500,  3.1875,  3.2500,  ...,  1.0469,  1.0469,  1.0469],\n",
       "         [ 5.6562,  2.8438, -2.3594,  ..., -0.5625, -0.5625, -0.5625]],\n",
       "\n",
       "        [[11.0000, 12.1875, 10.8750,  ...,  1.4297,  1.4297,  1.4297],\n",
       "         [11.0000, 12.1875, 10.8750,  ...,  1.4297,  1.4297,  1.4297],\n",
       "         [11.0000, 12.1875, 10.8750,  ...,  1.4297,  1.4297,  1.4297],\n",
       "         ...,\n",
       "         [10.2500,  6.6875,  8.6250,  ...,  3.2188,  3.2188,  3.2188],\n",
       "         [ 9.6250,  2.6719,  2.7188,  ...,  0.6719,  0.6719,  0.6719],\n",
       "         [ 5.3125,  2.3125, -3.1562,  ..., -0.8008, -0.8008, -0.8008]],\n",
       "\n",
       "        [[11.0000, 12.1875, 10.8750,  ...,  1.4297,  1.4297,  1.4297],\n",
       "         [11.0000, 17.6250, 17.5000,  ...,  8.5625,  8.5625,  8.5625],\n",
       "         [10.3750, 15.3125,  9.5000,  ...,  6.9062,  6.9062,  6.9062],\n",
       "         ...,\n",
       "         [10.3125,  6.5312,  8.0000,  ...,  3.2031,  3.2031,  3.2031],\n",
       "         [ 9.7500,  2.6094,  3.0469,  ...,  0.5469,  0.5469,  0.5469],\n",
       "         [ 5.8125,  3.4219, -1.2031,  ..., -0.3574, -0.3574, -0.3555]],\n",
       "\n",
       "        [[11.0000, 12.1875, 10.8750,  ...,  1.4297,  1.4297,  1.4297],\n",
       "         [11.0000, 17.6250, 17.5000,  ...,  8.5625,  8.5625,  8.5625],\n",
       "         [10.3750, 15.3125,  9.5000,  ...,  6.9062,  6.9062,  6.9062],\n",
       "         ...,\n",
       "         [10.6875,  7.5000,  9.8125,  ...,  3.3750,  3.3750,  3.3750],\n",
       "         [10.5000,  3.7188,  3.5625,  ...,  1.0625,  1.0625,  1.0625],\n",
       "         [ 6.2188,  3.4062, -1.4531,  ..., -0.4766, -0.4766, -0.4766]]],\n",
       "       device='cuda:0', grad_fn=<ToCopyBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x7f978129d8a0>, hidden_states=None, attentions=None, rope_deltas=tensor([[-761],\n",
       "        [-760],\n",
       "        [-763],\n",
       "        [-761],\n",
       "        [-761]], device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    input_ids=batch[\"input_ids\"].cuda(), \n",
    "    pixel_values=batch[\"pixel_values\"].to(dtype=model.dtype).cuda(),\n",
    "    heatmap_flat=batch['heatmap_flat'].to(dtype=model.dtype).cuda(),\n",
    "    **{key: batch[key].cuda() for key in ['attention_mask', 'image_grid_thw', 'labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000, 1028])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HeatmapEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, hidden_state: int):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(1, hidden_state)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.linear2 = nn.Linear(hidden_state, hidden_state)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h1 = self.linear1(x)\n",
    "        output = self.linear2(self.activation(h1))\n",
    "        \n",
    "        return output\n",
    "\n",
    "layer = TransformHiddenStateLayer(1028)\n",
    "out = layer(batch[\"heatmap_flat\"].view(-1, 1))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model\n",
    "output_attentions = None\n",
    "output_hidden_states = None\n",
    "return_dict = None\n",
    "inputs_embeds = None\n",
    "\n",
    "input_ids = batch[\"input_ids\"].cuda()\n",
    "pixel_values = batch[\"pixel_values\"].cuda()\n",
    "grid_thw=batch[\"image_grid_thw\"]\n",
    "\n",
    "\n",
    "output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "output_hidden_states = (\n",
    "    output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    ")\n",
    "return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "if inputs_embeds is None:\n",
    "    inputs_embeds = self.model.embed_tokens(input_ids)\n",
    "    if pixel_values is not None:\n",
    "        pixel_values = pixel_values.type(self.visual.dtype)\n",
    "        # heatmap_flat = heatmap_flat.reshape(-1, 1).type(self.visual.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VisionTransformerPretrainedModel(\n",
       "  (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "    (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "  )\n",
       "  (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "      (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "      (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "      (attn): Qwen2_5_VLVisionFlashAttention2(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp): Qwen2_5_VLMLP(\n",
       "        (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "        (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "        (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (merger): Qwen2_5_VLPatchMerger(\n",
       "    (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model.visual\n",
    "hidden_states = pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# from ...activations import ACT2FN\n",
    "# from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n",
    "# from ...generation import GenerationMixin\n",
    "# from ...modeling_attn_mask_utils import AttentionMaskConverter\n",
    "# from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n",
    "# from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n",
    "# from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
    "# from ...modeling_utils import PreTrainedModel\n",
    "# from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n",
    "# from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLVisionConfig\n",
    "\n",
    "\n",
    "hidden_states = self.patch_embed(hidden_states)\n",
    "rotary_pos_emb = self.rot_pos_emb(grid_thw)\n",
    "window_index, cu_window_seqlens = self.get_window_index(grid_thw)\n",
    "cu_window_seqlens = torch.tensor(\n",
    "    cu_window_seqlens,\n",
    "    device=hidden_states.device,\n",
    "    dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n",
    ")\n",
    "cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)\n",
    "\n",
    "seq_len, _ = hidden_states.size()\n",
    "hidden_states = hidden_states.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n",
    "hidden_states = hidden_states[window_index, :, :]\n",
    "hidden_states = hidden_states.reshape(seq_len, -1)\n",
    "rotary_pos_emb = rotary_pos_emb.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n",
    "rotary_pos_emb = rotary_pos_emb[window_index, :, :]\n",
    "rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)\n",
    "emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n",
    "position_embeddings = (emb.cos(), emb.sin())\n",
    "\n",
    "cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n",
    "    dim=0,\n",
    "    # Select dtype based on the following factors:\n",
    "    #  - FA2 requires that cu_seqlens_q must have dtype int32\n",
    "    #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw\n",
    "    # See https://github.com/huggingface/transformers/pull/34852 for more information\n",
    "    dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n",
    ")\n",
    "cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_seqlens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_num, blk in enumerate(self.blocks):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11]), torch.Size([501]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_seqlens.shape, cu_window_seqlens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000, 1280])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = blk(hidden_states, cu_seqlens=cu_window_seqlens, position_embeddings=position_embeddings)\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer_num, blk in enumerate(self.blocks):\n",
    "#     if layer_num in self.fullatt_block_indexes:\n",
    "#         cu_seqlens_now = cu_seqlens\n",
    "#     else:\n",
    "#         cu_seqlens_now = cu_window_seqlens\n",
    "#     if self.gradient_checkpointing and self.training:\n",
    "#         hidden_states = self._gradient_checkpointing_func(\n",
    "#             blk.__call__, hidden_states, cu_seqlens_now, None, position_embeddings\n",
    "#         )\n",
    "#     else:\n",
    "#         hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n",
    "\n",
    "# hidden_states = self.merger(hidden_states)\n",
    "# reverse_indices = torch.argsort(window_index)\n",
    "# hidden_states = hidden_states[reverse_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000, 1280])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLVisionBlock(\n",
       "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "  (attn): Qwen2_5_VLVisionFlashAttention2(\n",
       "    (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (mlp): Qwen2_5_VLMLP(\n",
       "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.blocks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HeatFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_features = torch.rand(hidden_states.shape, dtype=hidden_states.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4375, 0.9492, 0.4141,  ..., 0.8008, 0.9961, 0.5117],\n",
       "         [0.3008, 0.6992, 0.1992,  ..., 0.3906, 0.5195, 0.5820],\n",
       "         [0.9961, 0.3008, 0.1914,  ..., 0.8398, 0.0664, 0.1250],\n",
       "         ...,\n",
       "         [0.6133, 0.0625, 0.8594,  ..., 0.8633, 0.8828, 0.7188],\n",
       "         [0.8125, 0.3711, 0.5625,  ..., 0.5078, 0.7422, 0.2734],\n",
       "         [0.5000, 0.5156, 0.9766,  ..., 0.4570, 0.4258, 0.4297]],\n",
       "        dtype=torch.bfloat16),\n",
       " torch.Size([32000, 1280]),\n",
       " torch.Size([32000, 1280]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_features, context_features.shape, hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple\n",
    "import logging\n",
    "\n",
    "from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import flash_attn_varlen_func, apply_rotary_pos_emb_flashatt\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Qwen2_5_VLCrossAttentionFlashAttention2(nn.Module):\n",
    "    \"\"\"\n",
    "    Модифицированный Attention модуль для Cross-Attention с использованием FlashAttention v2.\n",
    "    Query (Q) берется из `context_features` (например, признаки тепловых карт).\n",
    "    Key (K) и Value (V) берутся из `hidden_states` (например, визуальные признаки).\n",
    "    Ротационные эмбеддинги (`position_embeddings`), предназначенные для `hidden_states`,\n",
    "    применяются к Q и K для сохранения структуры (требует совпадения длин!).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, dim_context: Optional[int] = None, num_heads: int = 16, bias: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        if dim % num_heads != 0:\n",
    "            raise ValueError(f\"`dim` ({dim}) должен быть кратен `num_heads` ({num_heads})\")\n",
    "\n",
    "        if dim_context is None:\n",
    "            dim_context = dim # По умолчанию контекст имеет ту же размерность, что и hidden_states\n",
    "\n",
    "        # Линейный слой для проекции context_features -> Q\n",
    "        # Вход: dim_context, Выход: dim (чтобы соответствовать K/V по размерности головы)\n",
    "        self.q_proj = nn.Linear(dim_context, dim, bias=bias)\n",
    "        # Линейный слой для проекции hidden_states -> K, V\n",
    "        # Вход: dim, Выход: dim * 2\n",
    "        self.kv_proj = nn.Linear(dim, dim * 2, bias=bias)\n",
    "        # Выходной проекционный слой\n",
    "        self.proj = nn.Linear(dim, dim) # Выходная размерность соответствует Q\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,                # Фичи для K, V, shape: (total_seq_len_kv, dim)\n",
    "        context_features: torch.Tensor,             # Фичи для Q, shape: (total_seq_len_q, dim_context)\n",
    "        cu_seqlens: torch.Tensor,                   # Кумулятивные длины для K/V (hidden_states), shape: (batch_size + 1,)\n",
    "        # cu_seqlens_context: torch.Tensor,           # Кумулятивные длины для Q (context), shape: (batch_size + 1,)\n",
    "        rotary_pos_emb: Optional[torch.Tensor] = None, # Устаревший способ передачи RoPE (для K/V)\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, # (cos, sin) для RoPE (для K/V)\n",
    "                                                                                # shape: (total_seq_len_kv, rotary_dim) или совместимые\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: Тензор для вычисления Key и Value. Форма (total_seq_len_kv, dim).\n",
    "            context_features: Тензор для вычисления Query. Форма (total_seq_len_q, dim_context).\n",
    "            cu_seqlens: Кумулятивные длины последовательностей для `hidden_states`. Форма (batch_size + 1,).\n",
    "            cu_seqlens_context: Кумулятивные длины последовательностей для `context_features`. Форма (batch_size + 1,).\n",
    "            rotary_pos_emb: Theta значения RoPE (устарело). Используется, если position_embeddings is None.\n",
    "            position_embeddings: Кортеж (cos, sin) для RoPE. Ожидается, что они рассчитаны для `hidden_states`.\n",
    "\n",
    "        Returns:\n",
    "            Тензор выхода attention. Форма (total_seq_len_q, dim).\n",
    "        \"\"\"\n",
    "        seq_length_kv = hidden_states.shape[0]     # Длина последовательности для K, V\n",
    "        seq_length_q = context_features.shape[0]   # Длина последовательности для Q\n",
    "\n",
    "        # --- ВАЖНАЯ ПРОВЕРКА ---\n",
    "        if seq_length_q != seq_length_kv:\n",
    "            logger.error(\n",
    "                f\"Длина последовательности для Q ({seq_length_q} из context_features) \"\n",
    "                f\"не совпадает с длиной для K/V ({seq_length_kv} из hidden_states). \"\n",
    "                f\"Применение одинаковых position_embeddings к Q и K в этом случае вызовет ошибку \"\n",
    "                f\"или будет некорректным! Убедитесь, что длины совпадают, или измените логику RoPE.\"\n",
    "            )\n",
    "            # Можно либо падать с ошибкой, либо продолжить с предупреждением,\n",
    "            # рискуя получить ошибку размерности в apply_rotary_pos_emb_flashatt\n",
    "            # raise ValueError(\"Sequence lengths for Q and K/V must match for this RoPE application strategy.\")\n",
    "            logger.warning(\"Продолжение работы с несовпадающими длинами Q и K/V, возможна ошибка в RoPE!\")\n",
    "\n",
    "\n",
    "        # 1. Проецируем K, V из hidden_states\n",
    "        # (total_kv, dim) -> (total_kv, 2 * dim) -> (total_kv, 2, num_heads, head_dim)\n",
    "        kv = self.kv_proj(hidden_states).reshape(seq_length_kv, 2, self.num_heads, self.head_dim)\n",
    "        # k, v shapes: (total_kv, num_heads, head_dim)\n",
    "        k, v = kv.unbind(1)\n",
    "        print(f\"CrossAttn: k shape: {k.shape}, v shape: {v.shape} (from hidden_states)\")\n",
    "\n",
    "        # 2. Проецируем Q из context_features\n",
    "        # (total_q, dim_context) -> (total_q, dim) -> (total_q, num_heads, head_dim)\n",
    "        q = self.q_proj(context_features).reshape(seq_length_q, self.num_heads, self.head_dim)\n",
    "        print(f\"CrossAttn: q shape: {q.shape} (from context_features)\")\n",
    "\n",
    "        # 3. Получаем и применяем RoPE (к Q и K, используя эмбеддинги от K/V)\n",
    "        if position_embeddings is None:\n",
    "            if rotary_pos_emb is None:\n",
    "                 raise ValueError(\"Необходимо предоставить либо position_embeddings, либо rotary_pos_emb\")\n",
    "            logger.warning_once(\n",
    "                \"Используется устаревший `rotary_pos_emb` для вычисления RoPE в CrossAttention.\"\n",
    "            )\n",
    "            # Убедимся, что rotary_pos_emb имеет правильную длину seq_length_kv\n",
    "            if rotary_pos_emb.shape[0] != seq_length_kv:\n",
    "                 raise ValueError(f\"rotary_pos_emb имеет длину {rotary_pos_emb.shape[0]}, ожидалось {seq_length_kv}\")\n",
    "            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        else:\n",
    "            cos, sin = position_embeddings\n",
    "            # Убедимся, что cos/sin имеют правильную длину seq_length_kv\n",
    "            if cos.shape[0] != seq_length_kv or sin.shape[0] != seq_length_kv:\n",
    "                 raise ValueError(f\"position_embeddings имеют длину {cos.shape[0]}/{sin.shape[0]}, ожидалось {seq_length_kv}\")\n",
    "\n",
    "        print(f\"CrossAttn: Применяем RoPE (cos: {cos.shape}, sin: {sin.shape}) к Q ({q.shape}) и K ({k.shape})\")\n",
    "        # Эта функция должна применить cos/sin (длины seq_length_kv) к q (длины seq_length_q) и k (длины seq_length_kv)\n",
    "        # Это будет работать без ошибок размерности только если seq_length_q == seq_length_kv\n",
    "        q, k = apply_rotary_pos_emb_flashatt(q.unsqueeze(0), k.unsqueeze(0), cos, sin)\n",
    "        print(f\"CrossAttn: q shape after RoPE: {q.shape}, k shape after RoPE: {k.shape}\")\n",
    "\n",
    "\n",
    "        # 4. Вычисляем максимальные длины последовательностей\n",
    "        # max_seqlen_q = (cu_seqlens_context[1:] - cu_seqlens_context[:-1]).max().item()\n",
    "        max_seqlen_kv = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n",
    "        print(f\"CrossAttn: max_seqlen_q: {max_seqlen_kv}, max_seqlen_kv: {max_seqlen_kv}\")\n",
    "\n",
    "        # 5. Вызываем FlashAttention для cross-attention\n",
    "        # q: (total_q, num_heads, head_dim)\n",
    "        # k: (total_kv, num_heads, head_dim)\n",
    "        # v: (total_kv, num_heads, head_dim)\n",
    "\n",
    "        q = q.squeeze(0)\n",
    "        k = k.squeeze(0)\n",
    "        print(\"q:\", q.shape, \"(total_q, num_heads, head_dim)\")\n",
    "        print(\"k:\", k.shape, \"(total_q, num_heads, head_dim)\")\n",
    "        print(\"v:\", v.shape, \"(total_q, num_heads, head_dim)\")\n",
    "\n",
    "        \n",
    "        attn_output = flash_attn_varlen_func(\n",
    "            q, k, v, cu_seqlens, cu_seqlens, max_seqlen_kv, max_seqlen_kv,\n",
    "            causal=False, # Cross-attention не каузальное\n",
    "        )\n",
    "        # attn_output shape: (total_q, num_heads, head_dim)\n",
    "        print(f\"CrossAttn: attn_output shape after flash_attn: {attn_output.shape}\")\n",
    "\n",
    "        # 6. Решейпим и проецируем выход\n",
    "        # (total_q, num_heads, head_dim) -> (total_q, dim)\n",
    "        attn_output = attn_output.reshape(seq_length_q, -1)\n",
    "        attn_output = self.proj(attn_output)\n",
    "        print(f\"CrossAttn: final attn_output shape after projection: {attn_output.shape}\")\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLVisionBlockHeat(\n",
       "  (norm0): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "  (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "  (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "  (attn): Qwen2_5_VLCrossAttentionFlashAttention2(\n",
       "    (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (kv_proj): Linear(in_features=1280, out_features=2560, bias=True)\n",
       "    (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (mlp): Qwen2_5_VLMLP(\n",
       "    (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "    (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "    (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2RMSNorm, Qwen2_5_VLMLP\n",
    "\n",
    "\n",
    "\n",
    "class Qwen2_5_VLVisionBlockHeat(nn.Module):\n",
    "    def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n",
    "        super().__init__()\n",
    "        self.norm0 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n",
    "        self.norm1 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n",
    "        self.norm2 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n",
    "        self.attn = Qwen2_5_VLCrossAttentionFlashAttention2(\n",
    "            config.hidden_size, num_heads=config.num_heads\n",
    "        )\n",
    "        self.mlp = Qwen2_5_VLMLP(config, bias=True)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        context_features: torch.Tensor,\n",
    "        cu_seqlens: torch.Tensor,\n",
    "        rotary_pos_emb: Optional[torch.Tensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        hidden_states = hidden_states + self.attn(\n",
    "            self.norm1(hidden_states),\n",
    "            self.norm0(context_features),\n",
    "            cu_seqlens=cu_seqlens,\n",
    "            rotary_pos_emb=rotary_pos_emb,\n",
    "            position_embeddings=position_embeddings,\n",
    "        )\n",
    "        hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "fa_heat = Qwen2_5_VLVisionBlockHeat(self.config)\n",
    "fa_heat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_heat = fa_heat.to(dtype=hidden_states.dtype).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossAttn: k shape: torch.Size([32000, 16, 80]), v shape: torch.Size([32000, 16, 80]) (from hidden_states)\n",
      "CrossAttn: q shape: torch.Size([32000, 16, 80]) (from context_features)\n",
      "CrossAttn: Применяем RoPE (cos: torch.Size([32000, 80]), sin: torch.Size([32000, 80])) к Q (torch.Size([32000, 16, 80])) и K (torch.Size([32000, 16, 80]))\n",
      "CrossAttn: q shape after RoPE: torch.Size([1, 32000, 16, 80]), k shape after RoPE: torch.Size([1, 32000, 16, 80])\n",
      "CrossAttn: max_seqlen_q: 64, max_seqlen_kv: 64\n",
      "q: torch.Size([32000, 16, 80]) (total_q, num_heads, head_dim)\n",
      "k: torch.Size([32000, 16, 80]) (total_q, num_heads, head_dim)\n",
      "v: torch.Size([32000, 16, 80]) (total_q, num_heads, head_dim)\n",
      "CrossAttn: attn_output shape after flash_attn: torch.Size([32000, 16, 80])\n",
      "CrossAttn: final attn_output shape after projection: torch.Size([32000, 1280])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4297, -0.2520, -0.0967,  ...,  0.1201,  0.2891,  0.2500],\n",
       "        [ 0.4297, -0.2520, -0.0967,  ...,  0.1201,  0.2891,  0.2500],\n",
       "        [ 0.4297, -0.2520, -0.0967,  ...,  0.1201,  0.2891,  0.2500],\n",
       "        ...,\n",
       "        [ 0.4297, -0.2520, -0.0967,  ...,  0.1201,  0.2891,  0.2500],\n",
       "        [ 0.4297, -0.2520, -0.0967,  ...,  0.1201,  0.2891,  0.2500],\n",
       "        [ 0.4297, -0.2520, -0.0967,  ...,  0.1201,  0.2891,  0.2500]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa_heat(hidden_states, context_features.cuda(), cu_seqlens=cu_window_seqlens, position_embeddings=position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32000, 1280])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После patch_embed, hidden_states shape: torch.Size([32000, 1280])\n",
      "После rot_pos_emb, rotary_pos_emb shape: torch.Size([32000, 40])\n",
      "После get_window_index, window_index shape: torch.Size([8000]), len(cu_window_seqlens_list): 661\n",
      "После torch.tensor, cu_window_seqlens shape: torch.Size([661]), dtype: torch.int32\n",
      "После unique_consecutive, cu_window_seqlens shape: torch.Size([501])\n",
      "Определены seq_len: 32000, hidden_dim: 1280\n",
      "hidden_states перед решейпом в окна: torch.Size([32000, 1280])\n",
      "hidden_states после решейпа в окна: torch.Size([8000, 4, 1280])\n",
      "hidden_states перед индексацией окнами (window_index): torch.Size([8000, 4, 1280])\n",
      "hidden_states после индексации окнами (перегруппировка): torch.Size([8000, 4, 1280])\n",
      "hidden_states перед финальным решейпом: torch.Size([8000, 4, 1280])\n",
      "hidden_states после финального решейпа (готово для блоков): torch.Size([32000, 1280])\n",
      "rotary_pos_emb перед решейпом в окна: torch.Size([32000, 40]) (seq_len_pos=32000, pos_dim=40)\n",
      "rotary_pos_emb после решейпа в окна: torch.Size([8000, 4, 40])\n",
      "rotary_pos_emb перед индексацией окнами (window_index): torch.Size([8000, 4, 40])\n",
      "rotary_pos_emb после индексации окнами (перегруппировка): torch.Size([8000, 4, 40])\n",
      "rotary_pos_emb перед финальным решейпом: torch.Size([8000, 4, 40])\n",
      "rotary_pos_emb после финального решейпа: torch.Size([32000, 40])\n",
      "rotary_pos_emb shape перед cat: torch.Size([32000, 40])\n",
      "emb shape после cat: torch.Size([32000, 80])\n",
      "position_embeddings[0] (cos) shape: torch.Size([32000, 80])\n",
      "position_embeddings[1] (sin) shape: torch.Size([32000, 80])\n",
      "grid_thw для full attention cu_seqlens: torch.Size([10, 3]), dtype: torch.int64\n",
      "Full attention: repeated_hw shape: torch.Size([10])\n",
      "Full attention: cu_seqlens (до pad) shape: torch.Size([10]), dtype: torch.int32\n",
      "Full attention: cu_seqlens (после pad) shape: torch.Size([11])\n",
      "\n",
      "--- Блок 0 ---\n",
      "Вход в блок 0, hidden_states shape: torch.Size([32000, 1280])\n",
      "Используем Windowed Attention cu_window_seqlens (форма torch.Size([501]))\n",
      "Выход из блока 0, hidden_states shape: torch.Size([32000, 1280])\n",
      "\n",
      "--- Блок 1 ---\n",
      "Вход в блок 1, hidden_states shape: torch.Size([32000, 1280])\n",
      "Используем Windowed Attention cu_window_seqlens (форма torch.Size([501]))\n",
      "Выход из блока 1, hidden_states shape: torch.Size([32000, 1280])\n",
      "\n",
      "--- Блок 2 ---\n",
      "Вход в блок 2, hidden_states shape: torch.Size([32000, 1280])\n",
      "Используем Windowed Attention cu_window_seqlens (форма torch.Size([501]))\n",
      "Выход из блока 2, hidden_states shape: torch.Size([32000, 1280])\n",
      "\n",
      "--- Блок 3 ---\n",
      "Вход в блок 3, hidden_states shape: torch.Size([32000, 1280])\n",
      "Используем Windowed Attention cu_window_seqlens (форма torch.Size([501]))\n",
      "Выход из блока 3, hidden_states shape: torch.Size([32000, 1280])\n",
      "\n",
      "--- Блок 4 ---\n",
      "Вход в блок 4, hidden_states shape: torch.Size([32000, 1280])\n",
      "Используем Windowed Attention cu_window_seqlens (форма torch.Size([501]))\n",
      "Выход из блока 4, hidden_states shape: torch.Size([32000, 1280])\n",
      "\n",
      "--- Блок 5 ---\n",
      "Вход в блок 5, hidden_states shape: torch.Size([32000, 1280])\n",
      "Используем Windowed Attention cu_window_seqlens (форма torch.Size([501]))\n",
      "Выход из блока 5, hidden_states shape: torch.Size([32000, 1280])\n",
      "\n",
      "--- Блок 6 ---\n",
      "Вход в блок 6, hidden_states shape: torch.Size([32000, 1280])\n",
      "Используем Windowed Attention cu_window_seqlens (форма torch.Size([501]))\n",
      "Выход из блока 6, hidden_states shape: torch.Size([32000, 1280])\n",
      "\n",
      "--- Блок 7 ---\n",
      "Вход в блок 7, hidden_states shape: torch.Size([32000, 1280])\n",
      "Используем Full Attention cu_seqlens (форма torch.Size([11]))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cu_seqlens_q must be on CUDA",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 139\u001b[0m\n\u001b[1;32m    134\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    135\u001b[0m             blk\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, cu_seqlens_now, \u001b[38;5;28;01mNone\u001b[39;00m, position_embeddings\n\u001b[1;32m    136\u001b[0m         )\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;66;03m# Передаем аргументы в блок\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens_now\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mВыход из блока \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, hidden_states shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_states\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- После всех блоков ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:339\u001b[0m, in \u001b[0;36mQwen2_5_VLVisionBlock.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    334\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m     position_embeddings: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    338\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 339\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states))\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:192\u001b[0m, in \u001b[0;36mQwen2_5_VLVisionFlashAttention2.forward\u001b[0;34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[0m\n\u001b[1;32m    189\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    191\u001b[0m max_seqlen \u001b[38;5;241m=\u001b[39m (cu_seqlens[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m cu_seqlens[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 192\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mflash_attn_varlen_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    193\u001b[0m     seq_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    194\u001b[0m )\n\u001b[1;32m    195\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(attn_output)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py:1448\u001b[0m, in \u001b[0;36mflash_attn_varlen_func\u001b[0;34m(q, k, v, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_attn_probs, block_table)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mflash_attn_varlen_func\u001b[39m(\n\u001b[1;32m   1376\u001b[0m     q,\n\u001b[1;32m   1377\u001b[0m     k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1391\u001b[0m     block_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1392\u001b[0m ):\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"dropout_p should be set to 0.0 during evaluation\u001b[39;00m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;124;03m    Supports multi-query and grouped-query attention (MQA/GQA) by passing in K, V with fewer heads\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;124;03m    than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03m            pattern (negative means that location was dropped, nonnegative means it was kept).\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlashAttnVarlenFunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attn_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py:930\u001b[0m, in \u001b[0;36mFlashAttnVarlenFunc.forward\u001b[0;34m(ctx, q, k, v, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_softmax, block_table, is_grad_enabled)\u001b[0m\n\u001b[1;32m    928\u001b[0m     k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(k, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m-\u001b[39m head_size_og \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m8\u001b[39m])\n\u001b[1;32m    929\u001b[0m     v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(v, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m-\u001b[39m head_size_og \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m8\u001b[39m])\n\u001b[0;32m--> 930\u001b[0m out_padded, softmax_lse, S_dmask, rng_state \u001b[38;5;241m=\u001b[39m \u001b[43m_wrapped_flash_attn_varlen_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size_left\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size_right\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_softmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_softmax\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_grad:\n\u001b[1;32m    949\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\n\u001b[1;32m    950\u001b[0m         q, k, v, out_padded, softmax_lse, cu_seqlens_q, cu_seqlens_k, rng_state\n\u001b[1;32m    951\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_library/autograd.py:113\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.autograd_impl\u001b[0;34m(keyset, *args, **keyword_only_args)\u001b[0m\n\u001b[1;32m    111\u001b[0m     result \u001b[38;5;241m=\u001b[39m Generated\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, Metadata(keyset, keyword_only_args))  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_no_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_only_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_library/autograd.py:40\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.forward_no_grad\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     38\u001b[0m keyset \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyset\n\u001b[1;32m     39\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyword_only_args\n\u001b[0;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_autograd_keyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py:721\u001b[0m, in \u001b[0;36mOpOverload.redispatch\u001b[0;34m(self, keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mredispatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch_boxed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_library/custom_ops.py:324\u001b[0m, in \u001b[0;36mCustomOpDef.register_kernel.<locals>.inner.<locals>.backend_impl\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackend_impl\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# Checks the assumption that outputs cannot alias\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# inputs or other outputs.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     storages \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28mid\u001b[39m(tensor\u001b[38;5;241m.\u001b[39muntyped_storage())\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m iter_tensors(args, kwargs)\n\u001b[1;32m    322\u001b[0m     }\n\u001b[0;32m--> 324\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_fns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     tuple_result \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_library/custom_ops.py:367\u001b[0m, in \u001b[0;36mCustomOpDef.register_kernel.<locals>.inner.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/flash_attn/flash_attn_interface.py:170\u001b[0m, in \u001b[0;36m_flash_attn_varlen_forward\u001b[0;34m(q, k, v, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, dropout_p, softmax_scale, causal, window_size_left, window_size_right, softcap, alibi_slopes, return_softmax, block_table, leftpad_k, seqused_k, zero_tensors)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;129m@_torch_custom_op_wrapper\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn::_flash_attn_varlen_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mutates_args\u001b[38;5;241m=\u001b[39m(), device_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_flash_attn_varlen_forward\u001b[39m(\n\u001b[1;32m    149\u001b[0m     q: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m     zero_tensors: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    168\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    169\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m [maybe_contiguous(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (q, k, v)]\n\u001b[0;32m--> 170\u001b[0m     out, softmax_lse, S_dmask, rng_state \u001b[38;5;241m=\u001b[39m \u001b[43mflash_attn_gpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvarlen_fwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqused_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleftpad_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size_left\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size_right\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_softmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# if out.isnan().any() or softmax_lse.isnan().any():\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m#     breakpoint()\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, softmax_lse, S_dmask, rng_state\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cu_seqlens_q must be on CUDA"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Предполагаем, что hidden_states и grid_thw уже существуют\n",
    "# print(f\"Начальный hidden_states shape: {hidden_states.shape}\") # Можно добавить, если нужно видеть вход\n",
    "# print(f\"Начальный grid_thw shape: {grid_thw.shape}\")      # Можно добавить, если нужно видеть вход\n",
    "\n",
    "hidden_states = self.patch_embed(hidden_states)\n",
    "print(f\"После patch_embed, hidden_states shape: {hidden_states.shape}\")\n",
    "\n",
    "rotary_pos_emb = self.rot_pos_emb(grid_thw)\n",
    "print(f\"После rot_pos_emb, rotary_pos_emb shape: {rotary_pos_emb.shape}\")\n",
    "# Убедимся что rotary_pos_emb на том же девайсе\n",
    "rotary_pos_emb = rotary_pos_emb.to(hidden_states.device, hidden_states.dtype)\n",
    "\n",
    "window_index, cu_window_seqlens_list = self.get_window_index(grid_thw)\n",
    "print(f\"После get_window_index, window_index shape: {window_index.shape}, len(cu_window_seqlens_list): {len(cu_window_seqlens_list)}\")\n",
    "# Перенесем window_index на нужный девайс, если он еще не там\n",
    "window_index = window_index.to(hidden_states.device)\n",
    "\n",
    "\n",
    "cu_window_seqlens = torch.tensor(\n",
    "    cu_window_seqlens_list, # Используем полученный список\n",
    "    device=hidden_states.device,\n",
    "    dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n",
    ")\n",
    "print(f\"После torch.tensor, cu_window_seqlens shape: {cu_window_seqlens.shape}, dtype: {cu_window_seqlens.dtype}\")\n",
    "\n",
    "cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)\n",
    "print(f\"После unique_consecutive, cu_window_seqlens shape: {cu_window_seqlens.shape}\")\n",
    "\n",
    "\n",
    "seq_len, hidden_dim = hidden_states.size() # Получаем размерность после patch_embed\n",
    "print(f\"Определены seq_len: {seq_len}, hidden_dim: {hidden_dim}\")\n",
    "\n",
    "# --- Решейпинг и индексация hidden_states ---\n",
    "print(f\"hidden_states перед решейпом в окна: {hidden_states.shape}\")\n",
    "# Проверка на делимость перед решейпом\n",
    "if seq_len % self.spatial_merge_unit != 0:\n",
    "    raise ValueError(f\"seq_len ({seq_len}) не делится на spatial_merge_unit ({self.spatial_merge_unit})\")\n",
    "hidden_states = hidden_states.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n",
    "print(f\"hidden_states после решейпа в окна: {hidden_states.shape}\")\n",
    "\n",
    "print(f\"hidden_states перед индексацией окнами (window_index): {hidden_states.shape}\")\n",
    "hidden_states = hidden_states[window_index, :, :]\n",
    "print(f\"hidden_states после индексации окнами (перегруппировка): {hidden_states.shape}\")\n",
    "\n",
    "print(f\"hidden_states перед финальным решейпом: {hidden_states.shape}\")\n",
    "hidden_states = hidden_states.reshape(seq_len, -1) # Используем seq_len и выводим hidden_dim\n",
    "print(f\"hidden_states после финального решейпа (готово для блоков): {hidden_states.shape}\")\n",
    "\n",
    "# --- Решейпинг и индексация rotary_pos_emb ---\n",
    "seq_len_pos, pos_dim = rotary_pos_emb.size()\n",
    "print(f\"rotary_pos_emb перед решейпом в окна: {rotary_pos_emb.shape} (seq_len_pos={seq_len_pos}, pos_dim={pos_dim})\")\n",
    "# Проверка совпадения seq_len (важно!)\n",
    "if seq_len_pos != seq_len:\n",
    "     # Можно добавить обработку ошибки или предупреждение\n",
    "     print(f\"[WARN/ERROR] seq_len у hidden_states ({seq_len}) не совпадает с seq_len у rotary_pos_emb ({seq_len_pos})!\")\n",
    "     # Попытка исправить размер rotary_pos_emb (может быть неверной логикой)\n",
    "     if rotary_pos_emb.shape[0] > seq_len:\n",
    "         rotary_pos_emb = rotary_pos_emb[:seq_len, :]\n",
    "     else:\n",
    "         padding = torch.zeros((seq_len - rotary_pos_emb.shape[0], pos_dim), dtype=rotary_pos_emb.dtype, device=rotary_pos_emb.device)\n",
    "         rotary_pos_emb = torch.cat([rotary_pos_emb, padding], dim=0)\n",
    "     print(f\"[FIX (POTENTIALLY WRONG)] Изменен размер rotary_pos_emb на: {rotary_pos_emb.shape}\")\n",
    "\n",
    "\n",
    "# Проверка на делимость перед решейпом\n",
    "if seq_len % self.spatial_merge_unit != 0:\n",
    "     # Эта ошибка должна была возникнуть раньше для hidden_states, но дублируем для ясности\n",
    "    raise ValueError(f\"seq_len ({seq_len}) не делится на spatial_merge_unit ({self.spatial_merge_unit}) для rotary_pos_emb\")\n",
    "rotary_pos_emb = rotary_pos_emb.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n",
    "print(f\"rotary_pos_emb после решейпа в окна: {rotary_pos_emb.shape}\")\n",
    "\n",
    "print(f\"rotary_pos_emb перед индексацией окнами (window_index): {rotary_pos_emb.shape}\")\n",
    "rotary_pos_emb = rotary_pos_emb[window_index, :, :]\n",
    "print(f\"rotary_pos_emb после индексации окнами (перегруппировка): {rotary_pos_emb.shape}\")\n",
    "\n",
    "print(f\"rotary_pos_emb перед финальным решейпом: {rotary_pos_emb.shape}\")\n",
    "rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1) # Используем seq_len и выводим pos_dim\n",
    "print(f\"rotary_pos_emb после финального решейпа: {rotary_pos_emb.shape}\")\n",
    "\n",
    "\n",
    "# --- Создание position_embeddings ---\n",
    "print(f\"rotary_pos_emb shape перед cat: {rotary_pos_emb.shape}\")\n",
    "emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n",
    "print(f\"emb shape после cat: {emb.shape}\")\n",
    "\n",
    "position_embeddings = (emb.cos(), emb.sin())\n",
    "print(f\"position_embeddings[0] (cos) shape: {position_embeddings[0].shape}\")\n",
    "print(f\"position_embeddings[1] (sin) shape: {position_embeddings[1].shape}\")\n",
    "\n",
    "# --- Вычисление cu_seqlens для full attention ---\n",
    "# Эта часть вычисляет cu_seqlens специфичным образом\n",
    "print(f\"grid_thw для full attention cu_seqlens: {grid_thw.shape}, dtype: {grid_thw.dtype}\")\n",
    "if grid_thw.dim() > 1 and grid_thw.shape[0] > 0 and grid_thw.shape[1] == 3:\n",
    "    # Вычисление на основе repeat_interleave\n",
    "    repeated_hw = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0].long()) # .long() для repeat_interleave\n",
    "    print(f\"Full attention: repeated_hw shape: {repeated_hw.shape}\")\n",
    "    cu_seqlens_full = repeated_hw.cumsum(\n",
    "        dim=0,\n",
    "        dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n",
    "    )\n",
    "    print(f\"Full attention: cu_seqlens (до pad) shape: {cu_seqlens_full.shape}, dtype: {cu_seqlens_full.dtype}\")\n",
    "\n",
    "    cu_seqlens_full = F.pad(cu_seqlens_full, (1, 0), value=0)\n",
    "    print(f\"Full attention: cu_seqlens (после pad) shape: {cu_seqlens_full.shape}\")\n",
    "else:\n",
    "    # Заглушка или обработка ошибки, если grid_thw не подходит\n",
    "    print(f\"[WARN] Не удалось вычислить full attention cu_seqlens из grid_thw формы {grid_thw.shape}. Создается заглушка.\")\n",
    "    # Стандартная заглушка для батча из 1 элемента\n",
    "    cu_seqlens_full = torch.tensor([0, seq_len], dtype=torch.int32, device=hidden_states.device)\n",
    "    print(f\"Full attention: cu_seqlens (заглушка) shape: {cu_seqlens_full.shape}\")\n",
    "\n",
    "\n",
    "# --- Цикл по блокам ---\n",
    "for layer_num, blk in enumerate(self.blocks):\n",
    "    print(f\"\\n--- Блок {layer_num} ---\")\n",
    "    print(f\"Вход в блок {layer_num}, hidden_states shape: {hidden_states.shape}\")\n",
    "\n",
    "    if layer_num in self.fullatt_block_indexes:\n",
    "        cu_seqlens_now = cu_seqlens_full # Используем рассчитанные выше\n",
    "        print(f\"Используем Full Attention cu_seqlens (форма {cu_seqlens_now.shape})\")\n",
    "    else:\n",
    "        cu_seqlens_now = cu_window_seqlens # Используем оконные\n",
    "        print(f\"Используем Windowed Attention cu_window_seqlens (форма {cu_seqlens_now.shape})\")\n",
    "\n",
    "    # Приведение типа для FA2, если не трассировка\n",
    "    if not torch.jit.is_tracing():\n",
    "        cu_seqlens_now = cu_seqlens_now.to(torch.int32)\n",
    "        # print(f\"Привели cu_seqlens_now к dtype: {cu_seqlens_now.dtype}\") # Можно раскомментировать для отладки\n",
    "\n",
    "    if self.gradient_checkpointing and self.training:\n",
    "        print(f\"Блок {layer_num}: Используем Gradient Checkpointing\")\n",
    "        hidden_states = self._gradient_checkpointing_func(\n",
    "            blk.__call__, hidden_states, cu_seqlens_now, None, position_embeddings\n",
    "        )\n",
    "    else:\n",
    "        # Передаем аргументы в блок\n",
    "        hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n",
    "\n",
    "    print(f\"Выход из блока {layer_num}, hidden_states shape: {hidden_states.shape}\")\n",
    "\n",
    "print(\"\\n--- После всех блоков ---\")\n",
    "print(f\"hidden_states перед merger: {hidden_states.shape}\")\n",
    "hidden_states = self.merger(hidden_states)\n",
    "print(f\"hidden_states после merger: {hidden_states.shape}\")\n",
    "\n",
    "# --- Восстановление порядка ---\n",
    "print(f\"hidden_states перед обратной индексацией: {hidden_states.shape}\")\n",
    "reverse_indices = torch.argsort(window_index)\n",
    "print(f\"reverse_indices shape: {reverse_indices.shape}\")\n",
    "hidden_states = hidden_states[reverse_indices, :]\n",
    "print(f\"hidden_states после обратной индексации (финальный выход сегмента): {hidden_states.shape}\")\n",
    "\n",
    "# return hidden_states # Возвращаем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  3200,  6400,  9600, 12800, 16000, 19200, 22400, 25600, 28800,\n",
       "        32000], dtype=torch.int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_seqlens_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
